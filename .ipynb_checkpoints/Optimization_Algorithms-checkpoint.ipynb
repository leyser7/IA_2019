{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Gradient descent "
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Gradient descent is a way to minimize an objective function J(θ)\n parameterized by a model's parameters θ ∈ R d by updating the parameters in the opposite direction of the gradient of the objective function \n∇\nθ\nJ\n(\nθ\n)\n w.r.t. to the parameters. The learning rate \nη\n determines the size of the steps we take to reach a (local) minimum."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Batch gradient descent"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Computes the gradient of the cost function w.r.t. to the parameters \nθ\n for the entire training dataset"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$θ = θ − η ⋅∇θ J(θ)$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "raw",
      "source": "for i in range(nb_epochs):\n  params_grad = evaluate_gradient(loss_function, data, params)\n  params = params - learning_rate * params_grad"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Stochastic gradient descent"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example \nx\n(\ni\n)\n and label \ny\n(\ni\n)\n:"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$θ\n=\nθ\n−\nη\n⋅\n∇\nθ\nJ\n(\nθ\n;\nx^\ni\n;\ny^i\n)$\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "raw",
      "source": "for i in range(nb_epochs):\n  np.random.shuffle(data)\n  for example in data:\n    params_grad = evaluate_gradient(loss_function, example, params)\n    params = params - learning_rate * params_grad"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Mini-batch gradient descent\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \nn\n training examples"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$ θ\n=\nθ\n−\nη\n⋅\n∇\nθ\nJ\n(\nθ\n;\nx\n(\ni\n:\ni\n+\nn\n)\n;\ny\n(\ni\n:\ni\n+\nn\n)$\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "raw",
      "source": "for i in range(nb_epochs):\n  np.random.shuffle(data)\n  for batch in get_batches(data, batch_size=50):\n    params_grad = evaluate_gradient(loss_function, batch, params)\n    params = params - learning_rate * params_grad"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Gradient descent optimization algorithms"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Momentum\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Momentum  is a method that helps accelerate SGD in the relevant direction and dampens oscillations\nIt does this by adding a fraction \nγ\n of the update vector of the past time step to the current update vector"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "$vt\n=\nγ*\n{v_{t−1}}\n+\nη\n∇\nθ\nJ\n(\nθ\n)\\\\\nθ\n=\nθ\n−\n{vt}$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "Some implementations exchange the signs in the equations. The momentum term \nγ\n is usually set to 0.9 or a similar value"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Nesterov accelerated gradient\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": " Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$v\nt\n=\nγ*\nv_{\nt\n−\n1}\n+\nη\n∇\nθ\nJ\n(\nθ\n−\nγ\n_{v\nt\n−\n1}\n)\n\\\\\nθ\n=\nθ\n−\nv\nt\n$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Adagrad"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "It adapts the learning rate to the parameters, performing smaller updates\n(i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$θ_{\nt\n+\n1\n,\ni}\n=\nθ_{\nt\n,\ni}\n−\n\\frac{n}{\n\\sqrt{\nG_{\nt\n,\ni\ni}\n+\nϵ}}\n⋅\ng\nt\n,\ni$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$G\nt\n∈\nR\n^{d\n×\nd}$\n here is a diagonal matrix where each diagonal element \ni\n,\ni\n is the sum of the squares of the gradients "
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Adadelta"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$Δ\nθ\nt\n=\n−\\frac{\nR\nM\nS\n[\nΔ\nθ\n]\nt\n−\n1}{\nR\nM\nS\n[\ng\n]\nt}\ng\nt\\\\\nθ\nt\n+\n1\n=\nθ\nt\n+\nΔ\nθ\nt$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# RMSprop"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \nγ\n to be set to 0.9, while a good default value for the learning rate \nη\n is 0.001."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$E\n[\ng\n^2\n]\nt\n=\n0.9\nE\n[\ng^\n2\n]_{\nt\n−\n1}\n+\n0.1\ng^\n2\nt\\\\\nθ\nt\n+\n1\n=\nθ\nt\n−\\frac{\nη}{\n\\sqrt{\nE\n[\ng\n2\n]\nt\n+\nϵ}}\ng\nt$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Adam"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $\nv\nt$\n like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m t $\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$m\nt\n=\nβ\n1\nm_{\nt\n−\n1}\n+\n(\n1\n−\nβ\n1\n)\ng\nt\\\\\nv\nt\n=\nβ_\n2\nv_{\nt\n−\n1}\n+\n(\n1\n−\nβ_\n2\n)\ng\nt^2$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "$\\hat{m}\nt\n=\\frac{\nm\nt}{\n1\n−\nβ^\nt\n1}\n\\\\\n\\hat{v}\nt\n=\n\\frac{v\nt}{\n1\n−\nβ\n^t\n2}\n$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "$θ_{\nt\n+\n1}\n=\nθ\nt\n−\\frac{\nη}{\n\\sqrt{\n\\hat{\nv}}\nt\n+\nϵ}\n\\hat{\nm}\n$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "default values of 0.9 for \nβ\n1\n, 0.999 for \nβ\n2\n, and \n10\n−\n8\n for \nϵ\n. "
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# AdaMax"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "ℓ\n∞\n also generally exhibits stable behavior. For this reason, the authors propose AdaMax (Kingma and Ba, 2015) and show that \nv\nt\n with \nℓ\n∞\n converges to the following more stable value."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "$u\nt\n=\nβ^\n∞\n2\nv\nt\n−\n1\n+\n(\n1\n−\nβ^\n∞\n2\n)\n|\ng\nt\n|^\n∞\n\\\\=\nmax\n(\nβ\n2\n⋅\nv\nt\n−\n1\n,\n|\ng\nt\n|\n)$\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "$θ\nt\n+\n1\n=\nθ\nt\n−\\frac{\nη}{\nu\nt}\n\\hat{\nm}\nt$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Good default values are again \nη\n=\n0.002\n, \nβ\n1\n=\n0.9\n, and \nβ\n2\n=\n0.999\n."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Nadam"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "Nadam (Nesterov-accelerated Adaptive Moment Estimation) thus combines Adam and NAG"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$θ\nt\n+\n1\n=\nθ\nt\n−\\frac{\nη}{\n\\sqrt{\n\\hat{\nv}\nt}\n+\nϵ}\n(\nβ\n1\n\\hat{\nm}\nt\n+\\frac{\n(\n1\n−\nβ\n1\n)\ng\nt}{\n1\n−\nβ^\nt\n1}\n)$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# AMSGrad"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "AMSGrad that uses the maximum of past squared gradients \nv\nt\n rather than the exponential average to update the parameters"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "$m\nt\n=\nβ\n1\nm\nt\n−\n1\n+\n(\n1\n−\nβ\n1\n)\ng\nt$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$v\nt\n=\nβ\n2\nv\nt\n−\n1\n+\n(\n1\n−\nβ\n2\n)\ng\n2\nt$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$\\hat\nv\nt\n=\nmax\n(\n\\hat\nv_{\nt\n−\n1}\n,\nv\nt\n)$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "$θ_{\nt\n+\n1}\n=\nθ\nt\n−\\frac{\nη}{\n\\sqrt{\n\\hat\nv\nt}\n}\nm\nt$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![alt text](perfomance.png \"Title\")"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class GD_optimizer:\n    \n    def __init__(self, f, x_init, y_init, optimizer='GD', alpha=0.01, gamma=0.9, eps=1e-8, beta=0.9, beta1=0.9, \n                 rho = 0.8, beta2=0.9):\n        self.x = x_init\n        self.y = y_init\n        self.v_x = 0\n        self.v_y = 0\n        self.m_x = 0\n        self.m_y = 0\n        self.num_updates = 0\n        self.x_h = []\n        self.y_h = []\n        self.e_h = []\n        self.f = f\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n        self.beta = beta\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.rho = rho\n        self.delta_x = 0\n        self.delta_y = 0\n        self.amsgrad_v_x_c = 0\n        self.amsgrad_v_y_c = 0\n        self.optimizer = optimizer\n        nmodels= \"gd\",\"momentum\", \"nag\", \"adagrad\", \"rmspro\", \"adam\",\"adadelta\", \"adamax\", \"nadam\", \"amsgrad\"\n        self.models = {\n            nmodels[0]: self.gd,\n            nmodels[1]: self.momentum,\n            nmodels[2]: self.nag,\n            nmodels[3]: self.adagrad,\n            nmodels[4]: self.rmspro,\n            nmodels[5]: self.adam,\n            nmodels[6]: self.adadelta,\n            nmodels[7]: self.adamax,\n            nmodels[8]: self.nadam,\n            nmodels[9]: self.amsgrad\n        } \n        \n    def gd(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        # update x and y\n        self.x = self.x - self.alpha*dz_dx\n        self.y = self.y - self.alpha*dz_dy\n    \n    def momentum(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        self.v_x = self.gamma * self.v_x + self.alpha * dz_dx \n        self.v_y = self.gamma * self.v_y + self.alpha * dz_dy\n        self.x = self.x - self.v_x  \n        self.y = self.y - self.v_y \n        \n    def adagrad(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        self.v_x += dz_dx**2\n        self.v_y += dz_dy**2\n        self.x -= (self.alpha / (np.sqrt(self.v_x + self.eps))) * dz_dx\n        self.y -= (self.alpha / (np.sqrt(self.v_y + self.eps))) * dz_dy      \n        \n    def rmspro(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        self.v_x = self.beta * self.v_x + (1 - self.beta) * dz_dx**2\n        self.v_y = self.beta * self.v_y + (1 - self.beta) * dz_dy**2\n        self.x -= (self.alpha / np.sqrt(self.v_x) + self.eps) * dz_dx\n        self.y -= (self.alpha / np.sqrt(self.v_y) + self.eps) * dz_dy\n        \n    def adadelta(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        \n        self.v_x = self.rho * self.v_x + (1. - self.rho) * dz_dx**2\n        \n        self.v_y = self.rho * self.v_y + (1. - self.rho) * dz_dy**2\n        \n        cur_delta_x = ((np.sqrt(self.delta_x + self.eps)) / (np.sqrt(self.v_x + self.eps))) * dz_dx\n        \n        cur_delta_y = ((np.sqrt(self.delta_y + self.eps)) / (np.sqrt(self.v_y + self.eps))) * dz_dy\n        \n        self.delta_x = self.rho * self.delta_x + (1. - self.rho) * cur_delta_x * cur_delta_x\n        \n        self.delta_y = self.rho * self.delta_y + (1. - self.rho) * cur_delta_y * cur_delta_y\n        \n        self.x -= cur_delta_x \n        \n        self.y -= cur_delta_y  \n        \n    def nag (self):\n        self.v_x = self.gamma * self.v_x\n        self.v_y = self.gamma * self.v_y\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x - self.v_x, self.y - self.v_y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x - self.v_x, self.y - self.v_y)\n        self.v_x = self.v_x + self.alpha * dz_dx\n        self.v_y = self.v_y + self.alpha * dz_dy\n        self.x = self.x - self.v_x  \n        self.y = self.y - self.v_y \n          \n    def adam(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        self.num_updates += 1\n        \n        self.m_x = self.beta1 * self.m_x + (1-self.beta1) * dz_dx\n        self.m_y = self.beta1 * self.m_y + (1-self.beta1) * dz_dy\n        \n        self.v_x = self.beta2 * self.v_x + (1-self.beta2) * dz_dx**2\n        self.v_y = self.beta2 * self.v_y + (1-self.beta2) * dz_dy**2\n        \n        m_x_c = self.m_x / (1 - np.power(self.beta1, self.num_updates))\n        m_y_c = self.m_y / (1 - np.power(self.beta1, self.num_updates))\n        \n        v_x_c = self.v_x / (1 - np.power(self.beta2, self.num_updates))\n        v_y_c = self.v_y / (1 - np.power(self.beta2, self.num_updates))\n        \n        \n        self.x -= (self.alpha / (np.sqrt(v_x_c) + self.eps)) * m_x_c\n        self.y -= (self.alpha / (np.sqrt(v_y_c) + self.eps)) * m_y_c\n        \n    def adamax(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        self.num_updates += 1\n        \n        self.m_x = self.beta1 * self.m_x + (1-self.beta1) * dz_dx\n        self.m_y = self.beta1 * self.m_y + (1-self.beta1) * dz_dy\n        \n        self.v_x = np.maximum(self.beta2 * self.v_x , abs(dz_dx))\n        self.v_y = np.maximum(self.beta2 * self.v_y , abs(dz_dy))\n        \n        m_x_c = self.m_x / (1 - np.power(self.beta1, self.num_updates))\n        m_y_c = self.m_y / (1 - np.power(self.beta1, self.num_updates))\n        \n        self.x -= (self.alpha / (self.v_x)) * m_x_c\n        self.y -= (self.alpha / (self.v_y)) * m_y_c\n        \n    def nadam(self):  \n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n        self.num_updates += 1\n        \n        self.m_x = self.beta1 * self.m_x + (1-self.beta1) * dz_dx\n        self.m_y = self.beta1 * self.m_y + (1-self.beta1) * dz_dy\n        \n        self.v_x = self.beta2 * self.v_x + (1-self.beta2) * dz_dx**2\n        self.v_y = self.beta2 * self.v_y + (1-self.beta2) * dz_dy**2\n        \n        beta1_sum = (1 - np.power(self.beta1, self.num_updates))\n        \n        m_x_c = self.m_x / beta1_sum\n        m_y_c = self.m_y / beta1_sum\n        \n        v_x_c = self.v_x / (1 - np.power(self.beta2, self.num_updates))\n        v_y_c = self.v_y / (1 - np.power(self.beta2, self.num_updates))\n        \n        self.x -= (self.alpha / (np.sqrt(v_x_c) + self.eps)) * (self.beta1 * m_x_c + (((1- self.beta1)*dz_dx)/ beta1_sum))\n        self.y -= (self.alpha / (np.sqrt(v_y_c) + self.eps)) * (self.beta1 * m_y_c + (((1- self.beta1)*dz_dy)/ beta1_sum))\n   \n    def amsgrad(self):\n        dz_dx = elementwise_grad(self.f, argnum=0)(self.x, self.y)\n        dz_dy = elementwise_grad(self.f, argnum=1)(self.x, self.y)\n\n        self.m_x = self.beta1 * self.m_x + (1-self.beta1) * dz_dx\n        self.m_y = self.beta1 * self.m_y + (1-self.beta1) * dz_dy\n        \n        self.v_x = self.beta2 * self.v_x + (1-self.beta2) * dz_dx**2\n        self.v_y = self.beta2 * self.v_y + (1-self.beta2) * dz_dy**2\n        \n        self.amsgrad_v_x_c = np.maximum(self.amsgrad_v_x_c, self.v_x)\n        self.amsgrad_v_y_c = np.maximum(self.amsgrad_v_y_c, self.v_y)\n        \n        self.x -= (self.alpha / (np.sqrt(self.amsgrad_v_x_c) + self.eps)) * self.m_x\n        self.y -= (self.alpha / (np.sqrt(self.amsgrad_v_y_c) + self.eps)) * self.m_y\n        \n    def fit(self, precision, epochs, verbose=0 ):\n        path_ = []\n        path_.append([self.x, self.y])\n        for e in range(epochs):\n            self.models[self.optimizer]()\n            # log the progress\n            if e % (epochs/10) == 0:\n                path_.append([self.x, self.y])\n                if self.f(self.x, self.y) == self.f(self.x, self.y):\n                    if verbose == 1 :\n                        print(\"epoch:\", e, \"derivate: \", self.f(self.x, self.y)) \n                else :\n                    print(\"Error\") \n                    break\n            if( abs(self.f(self.x, self. y))  < precision):\n                break\n            path = np.array(path_).T\n        print(self.f(self.x, self. y))\n        return self.x, self.y, e, path",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import LogNorm\n\n\n!pip install autograd\nfrom autograd import elementwise_grad, value_and_grad\nfrom collections import defaultdict\nfrom itertools import zip_longest\nfrom matplotlib import animation\nfrom IPython.display import HTML\nfrom random import randrange",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Beale's Function\n\\begin{equation*}\nf(x,y)=(1.5−x+xy)^2+(2.25−x+xy^2)^2+(2.625−x+xy^3)^2\n\\end{equation*}"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "xmin, xmax, xstep = -4.5, 4.5, .2\nymin, ymax, ystep = -4.5, 4.5, .2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "z = f(x, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Global minima is at  (3,0.5)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "minima = np.array([3., .5])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "f(*minima)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "minima_ = minima.reshape(-1, 1)\nminima_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "f(*minima_)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(8, 5))\nax = plt.axes(projection='3d', elev=50, azim=-50)\n\nax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, \n                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\nax.plot(*minima_, f(*minima_), 'r*', markersize=10)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_zlabel('$z$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dz_dx = elementwise_grad(f, argnum=0)(x, y)\ndz_dy = elementwise_grad(f, argnum=1)(x, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\nax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\nax.quiver(x, y, x - dz_dx, y - dz_dy, alpha=.5)\nax.plot(*minima_, 'r*', markersize=18)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#x0 = [randrange(-400,400,1)/100.0, randrange(-400,400,1)/100.0]\nx0 = [-3.4, 2.2]\ngd = GD_optimizer(f, x0[0],  x0[1], optimizer = 'gd', alpha=0.001)\nx_0, y_0, e, path  = gd.fit(precision=0.001, epochs=1000)\nprint (\"Minimum: \" + str(f(x_0,y_0)) + \" at (\" + str(x_0) + \",\" + str(y_0) + \")\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\nax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\nax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n\nax.plot(*minima_, 'r*', markersize=18)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(8, 5))\nax = plt.axes(projection='3d', elev=50, azim=-50)\n\nax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)\nax.quiver(path[0,:-1], path[1,:-1], f(*path[::,:-1]), \n          path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], f(*(path[::,1:]-path[::,:-1])), \n          color='k')\nax.plot(*minima_, f(*minima_), 'r*', markersize=10)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_zlabel('$z$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\nax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\nax.plot(*minima_, 'r*', markersize=18)\n\nline, = ax.plot([], [], 'b', label='GD', lw=2)\npoint, = ax.plot([], [], 'bo')\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))\n\nax.legend(loc='upper left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def init():\n    line.set_data([], [])\n    point.set_data([], [])\n    return line, point",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def animate(i):\n    line.set_data(*path[::,:i])\n    point.set_data(*path[::,i-1:i])\n    return line, point",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "anim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=path.shape[1], interval=200, \n                               repeat_delay=5, blit=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "HTML(anim.to_jshtml())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(8, 5))\nax = plt.axes(projection='3d', elev=50, azim=-50)\n\nax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)\nax.plot(*minima_, f(*minima_), 'r*', markersize=10)\n\nline, = ax.plot([], [], [], 'b', label='GD', lw=2)\npoint, = ax.plot([], [], [], 'bo')\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_zlabel('$z$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def init():\n    line.set_data([], [])\n    line.set_3d_properties([])\n    point.set_data([], [])\n    point.set_3d_properties([])\n    return line, point",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def animate(i):\n    line.set_data(path[0,:i], path[1,:i])\n    line.set_3d_properties(f(*path[::,:i]))\n    point.set_data(path[0,i-1:i], path[1,i-1:i])\n    point.set_3d_properties(f(*path[::,i-1:i]))\n    return line, point",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "anim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=path.shape[1], interval=200, \n                               repeat_delay=5, blit=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "HTML(anim.to_jshtml())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class TrajectoryAnimation(animation.FuncAnimation):\n    \n    def __init__(self, *paths, labels=[], fig=None, ax=None, frames=None, \n                 interval=300, repeat_delay=5, blit=True, **kwargs):\n\n        if fig is None:\n            if ax is None:\n                fig, ax = plt.subplots()\n            else:\n                fig = ax.get_figure()\n        else:\n            if ax is None:\n                ax = fig.gca()\n\n        self.fig = fig\n        self.ax = ax\n        \n        self.paths = paths\n\n        if frames is None:\n            frames = max(path.shape[1] for path in paths)\n  \n        self.lines = [ax.plot([], [], label=label, lw=2)[0] \n                      for _, label in zip_longest(paths, labels)]\n        self.points = [ax.plot([], [], 'o', color=line.get_color())[0] \n                       for line in self.lines]\n\n        super(TrajectoryAnimation, self).__init__(fig, self.animate, init_func=self.init_anim,\n                                                  frames=frames, interval=interval, blit=blit,\n                                                  repeat_delay=repeat_delay, **kwargs)\n\n    def init_anim(self):\n        for line, point in zip(self.lines, self.points):\n            line.set_data([], [])\n            point.set_data([], [])\n        return self.lines + self.points\n\n    def animate(self, i):\n        for line, point, path in zip(self.lines, self.points, self.paths):\n            line.set_data(*path[::,:i])\n            point.set_data(*path[::,i-1:i])\n        return self.lines + self.points",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class TrajectoryAnimation3D(animation.FuncAnimation):\n    \n    def __init__(self, *paths, zpaths, labels=[], fig=None, ax=None, frames=None, \n                 interval=300, repeat_delay=5, blit=True, **kwargs):\n\n        if fig is None:\n            if ax is None:\n                fig, ax = plt.subplots()\n            else:\n                fig = ax.get_figure()\n        else:\n            if ax is None:\n                ax = fig.gca()\n\n        self.fig = fig\n        self.ax = ax\n        \n        self.paths = paths\n        self.zpaths = zpaths\n        \n        if frames is None:\n            frames = max(path.shape[1] for path in paths)\n  \n        self.lines = [ax.plot([], [], [], label=label, lw=2)[0] \n                      for _, label in zip_longest(paths, labels)]\n\n        super(TrajectoryAnimation3D, self).__init__(fig, self.animate, init_func=self.init_anim,\n                                                  frames=frames, interval=interval, blit=blit,\n                                                  repeat_delay=repeat_delay, **kwargs)\n\n    def init_anim(self):\n        for line in self.lines:\n            line.set_data([], [])\n            line.set_3d_properties([])\n        return self.lines\n\n    def animate(self, i):\n        for line, path, zpath in zip(self.lines, self.paths, self.zpaths):\n            line.set_data(*path[::,:i])\n            line.set_3d_properties(zpath[:i])\n        return self.lines",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "methods = [\n    \"gd\"      ,\n    \"momentum\",\n    \"nag\"     ,\n    \"adagrad\" ,\n    \"rmspro\"  ,\n    \"adadelta\",\n    \"adam\",\n    \"adamax\",\n    \"nadam\",\n    \"amsgrad\"\n]\nalpha = {\n    \"gd\"       : 0.001,       \n    \"momentum\" : 0.001,\n    \"nag\"      : 0.001,\n    \"adagrad\"  : 0.8,\n    \"rmspro\"   : 0.001,\n    \"adam\"     : 0.001,\n    \"adadelta\" : 0.001,\n    \"adamax\"   : 0.001,\n    \"nadam\"    : 0.001,\n    \"amsgrad\"  : 0.8\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#x0 = [randrange(-400,400,1)/100.0, randrange(-400,400,1)/100.0]\n#x0 = [1.75, 1.16]\nx0 = [1.75, 2.5]\npaths = defaultdict(list)\nfor method in methods:\n    print (method)\n    gd = GD_optimizer(f, x0[0],  x0[1], optimizer = method, alpha=alpha[method])\n    x_0, y_0, e, paths[method] = gd.fit(precision=0.0, epochs=5000)\npaths = [np.array(paths[method]) for method in methods]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\nax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\nax.plot(*minima_, 'r*', markersize=10)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))\n\nanim = TrajectoryAnimation(*paths, labels=methods, ax=ax)\n\nax.legend(loc='upper left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "HTML(anim.to_jshtml())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(8, 5))\nax = plt.axes(projection='3d', elev=50, azim=-50)\n\nax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)\nax.plot(*minima_, f(*minima_), 'r*', markersize=10)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_zlabel('$z$')\n\nax.set_xlim((xmin, xmax))\nax.set_ylim((ymin, ymax))\n\nzpaths = [f(*path) for path in paths]\nanim = TrajectoryAnimation3D(*paths, zpaths=zpaths, labels=methods, ax=ax)\n\nax.legend(loc='upper left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "HTML(anim.to_jshtml())",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}